{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Self-RAG + LangGraph 例子"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6ed6017674bb221"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "! pip install -U langgraph langchain-openai tavily-python langchain-community tiktoken langchainhub chromadb langchain langgraph\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee66ad948f5c9901",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 初始化 langsmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"default\"\n",
    "\n",
    "# https://smith.langchain.com\n",
    "\n",
    "# Tavily API Key\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "# https://app.tavily.com/\n",
    "\n",
    "# 加载 .env 文件\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc7f697334023714",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain import hub as langchain_hub\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from string import Template\n",
    "import uuid\n",
    "\n",
    "# 读取 ./data/data.md 文件作为知识库\n",
    "file_path = os.path.join('/Users/chenchen/Desktop/egg/aiops/module_5/demo_6/data', 'data.md')\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    docs_string = f.read()\n",
    "\n",
    "# split the document into chunks base on markdown headers\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "splits = text_splitter.split_text(docs_string)\n",
    "print(\"Length of splits: \", len(splits))\n",
    "print(splits)\n",
    "\n",
    "# 向量化\n",
    "# 保存到随机目录里\n",
    "random_directory = \"./\" + str(uuid.uuid4())\n",
    "# embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\",\n",
    "#                              openai_api_key=os.getenv(\"OPENAI_API_KEY\"), openai_api_base=os.getenv(\"OPENAI_API_BASE\"))\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding, persist_directory=random_directory)\n",
    "vectorstore.persist()\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec601bc30667f3b8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 评估检索的文档与用户提出的问题是否相关\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                 openai_api_base=os.getenv(\"OPENAI_API_BASE\"))\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# prompt\n",
    "system = \"\"\"\n",
    "您是一名评分员，负责评估检索到的文档与用户问题的相关性。\\n\n",
    "测试不需要很严格，目标是过滤掉错误的检索。\\n\n",
    "如果文档包含与用户问题相关的关键字或语义含义，则将其评为相关。\\n\n",
    "给出二进制分数\"yes\"或\"no\"，以指示文档是否与问题相关。\n",
    "\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: \\n\\n {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "# 相关问题\n",
    "question = \"payment_backend 服务是谁维护的\"\n",
    "# 不相关问题\n",
    "# question = \"北京天气如何\"\n",
    "\n",
    "docs = retriever._get_relevant_documents(question, run_manager=None)\n",
    "\n",
    "# 观察每一个文档块的相关性判断结果\n",
    "for doc in docs:\n",
    "    print(\"doc: \\n\", doc.page_content, \"\\n\")\n",
    "    print(retrieval_grader.invoke({\"question\": question, \"document\": doc.page_content}))\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8ce324d2e00ab7e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 生成回复\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                 openai_api_base=os.getenv(\"OPENAI_API_BASE\"))\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cf7bf799d628fa8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 评估 LLM 的回复是否基于事实，有没有产生幻觉\n",
    "\n",
    "class GraderHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucinations present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                 openai_api_base=os.getenv(\"OPENAI_API_BASE\"))\n",
    "structured_llm_grader = llm.with_structured_output(GraderHallucinations)\n",
    "\n",
    "# prompt\n",
    "system = \"\"\"\n",
    "您是一名评分员，正在评估 LLM 生成是否基于一组检索的事实/由一组检索到的事实支持。\\n\n",
    "给出二进制分数\"yes\"或\"no\"，\"yes\"表示答案基于一组事实/由一组事实支持。\n",
    "\"\"\"\n",
    "\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generations: {generation}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b80162d8dbd81bd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 评估 LLM 回答是否解决了用户的问题\n",
    "\n",
    "class GraderAnswer(BaseModel):\n",
    "    \"\"\"Binary score to access answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                 openai_api_base=os.getenv(\"OPENAI_API_BASE\"))\n",
    "structured_llm_grader = llm.with_structured_output(GraderAnswer)\n",
    "\n",
    "# prompt\n",
    "system = \"\"\"\n",
    "您是一名评分员，评估答案是否解决了某个问题。\\n\n",
    "给出二进制分数\"yes\"或\"no\"，\"yes\"表示答案解决了问题。\n",
    "\"\"\"\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generations: {generation}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader\n",
    "\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83e5b347f1819fff",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 重写问题\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                 openai_api_base=os.getenv(\"OPENAI_API_BASE\"))\n",
    "\n",
    "system = \"\"\"\n",
    "您有一个问题重写器，可将输入问题转换为针对 vectorstore 检索进行了优化的更好版本 \\n\n",
    "查看输入并尝试推断底层语义意图/含义，使用用户语言回复。\n",
    "\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8c92748855291c3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# langGraph 构造 Agent\n",
    "\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "    \n",
    "    Attributes；\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "# 判断检索到的文档是否和问题相关\n",
    "def grade_document(state):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"文档和用户问题相关\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"文档和用户问题不相关\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"改写问题\\n\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    print(\"LLM 改写优化后更好的问题:\", better_question)\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "\n",
    "# Edge\n",
    "def decide_to_generate(state):\n",
    "    print(\"访问检索到的相关知识库\\n\")\n",
    "    filtered_documents = state[\"documents\"]\n",
    "    if not filtered_documents:\n",
    "        print(\"所有的文档都不相关，重写问题\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"文档跟问题相关，生成回答\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "# 评估生成的回复是否基于知识库\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    print(\"评估生成的回答是否基于知识库事实(是否产生了幻觉)\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\n",
    "        \"documents\": documents, \"generation\": generation\n",
    "    })\n",
    "    grade = score.binary_score\n",
    "    if grade == \"yes\":\n",
    "        print(\"生成的回复基于知识库，没有幻觉\\n\")\n",
    "        # 评估 LLM 的回答是否解决了用户问题\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"问题得到解决\\n\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"问题没有得到解决\\n\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"生成的回复不是基于知识库，继续重试...\\n\")\n",
    "        return \"not supported\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c96cb9f9e5ad3985",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_document)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"transform_query\", transform_query)\n",
    "\n",
    "# Graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    }\n",
    ")\n",
    "app = workflow.compile()\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5ec911a7c07f994",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Node 'retrieve': \"\n",
      "'\\n--\\n'\n",
      "文档和用户问题相关\n",
      "文档和用户问题不相关\n",
      "文档和用户问题不相关\n",
      "文档和用户问题不相关\n",
      "文档跟问题相关，生成回答\n",
      "\"Node 'grade_documents': \"\n",
      "'\\n--\\n'\n",
      "评估生成的回答是否基于知识库事实(是否产生了幻觉)\n",
      "生成的回复基于知识库，没有幻觉\n",
      "问题得到解决\n",
      "\n",
      "\"Node 'generate': \"\n",
      "'\\n--\\n'\n",
      "'小王管理的服务最多，共管理两个系统：payment 和 payment-1。其他人管理的服务数量较少。小李和小张各自只管理一个系统。'\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"question\": \"谁管理的服务最多\"}\n",
    "\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Node '{key}': \")\n",
    "\n",
    "    pprint(\"\\n--\\n\")\n",
    "\n",
    "pprint(value[\"generation\"])       "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-07T10:11:20.382265Z",
     "start_time": "2025-02-07T10:08:41.009990Z"
    }
   },
   "id": "a65fc169d25420be",
   "execution_count": 63
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
